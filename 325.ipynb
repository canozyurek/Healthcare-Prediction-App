{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc55b37-ecd6-4afe-84fb-12778a6a41dc",
   "metadata": {},
   "source": [
    "# Module 3: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7595f8-c43e-480d-ad2b-b89afbdc9514",
   "metadata": {},
   "source": [
    "## Sprint 2: Gradient Boosted Trees & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316343c-e474-4441-989a-a0b30ce11f27",
   "metadata": {},
   "source": [
    "## Part 5: Stroke Prediction Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3969567-e6d7-41fd-8a97-2f51c04b10c0",
   "metadata": {},
   "source": [
    "## About this Part\n\nCongrats!\nYou have reached the last Part of this Sprint.\nIn this Part, you will put what you learned during this and the previous Sprints into practice.\nAs the final assignment of this Sprint, you will train and deploy multiple machine learning models on the Stroke Prediction Dataset.\nYou will have to apply all that you have learned about training and deploying machine learning models to complete this task.\nOur expectation is that you'll use your own judgment on how to perform the analysis and how to select the most important avenues of modeling, statistical testing, and exploration.\nYou'll have to iteratively try to find patterns in the data, raise hypotheses and use your data analysis skills to get answers.\n\nP.S. we don't expect this project to be perfect - you will continue to improve your skills, and there will be many projects for you to apply your newly gained skills in the future.\nFor now, just use what you have learned and try your best!\n\n## Context\n\nImagine that you are a data analyst working for The Johns Hopkins Hospital.\nYour team is asked to create a machine learning model, which could predict if the patient is likely to get a stroke - being able to determine which patients have high stroke risk will allow your doctors to advise them and their families on how to act in case of an emergency.\nAs you are about to start working on your project, your team's manager approaches you with a request.\nShe has read an article that AI can predict all sorts of things that required dedicated sensors before - [Tesla doesn't use a rain sensor for its automatic wiper system](https://electrek.co/2019/10/14/tesla-deep-rain-neural-net-automatic-wipers/).\nFollowing the example of Elon Musk, she proposes her brilliant idea to save money - to replace all blood pressure monitors, glucose monitors, and scales with AI.\nShe argues that this would save the hospital millions of dollars every year.\nShe wants you to create three additional machine learning models to predict hypertension, average glucose level, and BMI of the patient.\nYou are not 100% convinced of this idea and try to explain why it won't work, but she is insistent, and you promise to try anyway - this will give you a great opportunity to practice your new machine learning skills and have metrics about how accurate predictions for these variables can be using state-of-the-art machine learning models.\n\n## Objectives for this Part\n\n- Practice working with CSV files.\n- Practice performing EDA.\n- Practice applying statistical inference procedures.\n- Practice using various types of machine learning models.\n- Practice building ensembles of machine learning models.\n- Practice deploying machine learning models.\n- Practice visualizing data with Matplotlib & Seaborn.\n- Practice reading data, performing queries, and filtering data.\n\n## Requirements\n\n- Download the data from [Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset).\n- Perform exploratory data analysis. This should include creating statistical summaries and charts, testing for anomalies, and checking for correlations and other relations between variables and other EDA elements.\n- Perform statistical inference. This should include defining the target population, forming multiple statistical hypotheses and constructing confidence intervals, setting the significance levels, and conducting z or t-tests for these hypotheses.\n- Apply various machine learning models to predict the \"stroke\" column using all other features. This should include hyperparameter tuning, model ensembling, the analysis of model selection, and other methods.\n- In addition, build several other machine learning models which predict other features. Your modeling process should include hyperparameter tuning, the analysis of model selection, and other methods. Build these models to predict:\n\t- The \"hypertension\" column. You are allowed to use all other features except for \"stroke\".\n\t- The \"avg_glucose_level\" column. You are allowed to use all other features except for \"stroke\".\n\t- The \"bmi\" column. You are allowed to use all other features except for \"stroke\".\n\t- The \"hypertension\" and \"avg_glucose_level\" columns. This will either be a model that predicts both of these columns at the same time or two models that each predict one of these features. Remember that if you are using two models, you are not allowed to use these columns in either of your models.\n\t- The \"hypertension\" and \"bmi\" columns. This will either be a model that predicts both of these columns at the same time or two models that each predict one of these features. Remember that if you are using two models, you are not allowed to use these columns in either of your models.\n\t- The \"avg_glucose_level\" and \"bmi\" columns. This will either be a model that predicts both of these columns at the same time or two models that each predict one of these features. Remember that if you are using two models, you are not allowed to use these columns in either of your models.\n\t- The \"hypertension\", \"avg_glucose_level\", and \"bmi\" columns. This will either be a model that predicts all three of these columns at the same time or three models that each predict one of these features. Remember that if you are using three models, you are not allowed to use these columns in either of your models.\n- Deploy all of these machine learning models. You are free to choose any deployment option that you like - you can deploy your models in a container (on your computer or on a server), do a serverless deployment on the cloud, or even deploy and serve it on the browser as a web app. These deployments don't have to be separate, as long as each model has its own endpoint.\n- Provide clear explanations in your notebook. Your explanations should inform the reader what you are trying to achieve, what results did you get, and what these results mean.\n- Provide suggestions about how your analysis can be improved.\n\n## Evaluation Criteria\n\n- Adherence to the requirements. How well did you meet the requirements?\n- Depth of your analysis. Did you just skim the surface, or did you explore the dataset in-depth?\n- Model's performance. How well did your model perform the predictions?\n- Visualization quality. Did you use charts effectively to visualize patterns in the data? Are your visualizations properly labeled? Did you use colors effectively? Did you adhere to the principle of proportional ink?\n- Code quality. Was your code well-structured? Did you use the appropriate levels of abstraction? Did you remove commented-out and unused code? Did you adhere to the PEP8?\n- Code performance. Did you use suitable algorithms and data structures to solve the problems?\n\n## Correction\n\nDuring your project correction, you should present your project as if talking to a product manager and senior data analyst working in your team.\nYou will have to find the right balance between explaining the business side and the technical aspects of your work.\nYou can assume that both of your colleagues have a strong understanding of and are very interested in the business aspect of your project, so be sure to clearly explain what new insights you've found while analyzing the dataset and which directions look the most promising for further research. \nHowever, you should also spend time explaining the technical aspects of your work, especially the more complex or unconventional choices.\n\nDuring a correction, you may get asked questions that test your understanding of covered topics.\n\n- What is wrong with preprocessing data before we run a hyperparameter search algorithm (e.g., randomized search) with cross-validation?\n- What are the typical model deployment patterns? What are their advantages and disadvantages?\n- How do hidden feedback loops affect machine learning models? Can you give an example of a hidden feedback loop?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
